{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dfc46fb",
   "metadata": {},
   "source": [
    "# Optional Challenge: Ames Housing Price Prediction\n",
    "\n",
    "This is an **optional** task for students who want an extra challenge.\n",
    "\n",
    "You will build a model to predict **SalePrice** for houses in Ames, Iowa using **79 explanatory variables**.\n",
    "\n",
    "**Skills to practice**\n",
    "- Leakage-safe preprocessing with `Pipeline` + `ColumnTransformer`\n",
    "- Creative feature engineering (domain-inspired features)\n",
    "- Advanced regression techniques (start with Linear Regression or Decision Tree baseline, then Random Forest, then tune)\n",
    "- Cross-validation and proper evaluation (RMSLE / RMSE on log-scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9820d5",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Files (already in this repo):\n",
    "- `datasets/housing/optional/train.csv` (has `SalePrice`)\n",
    "- `datasets/housing/optional/test.csv` (no `SalePrice`)\n",
    "- `datasets/housing/optional/data_description.txt` (column definitions)\n",
    "\n",
    "**Goal**: Train on `train.csv`, then evaluate your model using cross-validation. You can also predict on `test.csv` to see how your model performs on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61cc6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f18e6afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (1460, 81)\n",
      "test shape : (1459, 80)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_DIR = Path('datasets/housing/optional')\n",
    "train_path = DATA_DIR / 'train.csv'\n",
    "test_path = DATA_DIR / 'test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "print('train shape:', train_df.shape)\n",
    "print('test shape :', test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e762206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SalePrice statistics:\n",
      "count      1460.000000\n",
      "mean     180921.195890\n",
      "std       79442.502883\n",
      "min       34900.000000\n",
      "25%      129975.000000\n",
      "50%      163000.000000\n",
      "75%      214000.000000\n",
      "max      755000.000000\n",
      "Name: SalePrice, dtype: float64\n",
      "\n",
      "Skewness: 1.883\n",
      "\n",
      "Top 15 columns with missing values:\n",
      "PoolQC          99.520548\n",
      "MiscFeature     96.301370\n",
      "Alley           93.767123\n",
      "Fence           80.753425\n",
      "MasVnrType      59.726027\n",
      "FireplaceQu     47.260274\n",
      "LotFrontage     17.739726\n",
      "GarageQual       5.547945\n",
      "GarageFinish     5.547945\n",
      "GarageType       5.547945\n",
      "GarageYrBlt      5.547945\n",
      "GarageCond       5.547945\n",
      "BsmtFinType2     2.602740\n",
      "BsmtExposure     2.602740\n",
      "BsmtCond         2.534247\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Basic checks\n",
    "assert 'SalePrice' in train_df.columns, 'Expected target column SalePrice in train.csv'\n",
    "assert 'SalePrice' not in test_df.columns, 'test.csv should not include SalePrice'\n",
    "\n",
    "# Peek at target distribution\n",
    "print(f'SalePrice statistics:')\n",
    "print(train_df['SalePrice'].describe())\n",
    "print(f'\\nSkewness: {train_df[\"SalePrice\"].skew():.3f}')\n",
    "\n",
    "# Optional: peek at missingness\n",
    "missing = (train_df.isna().mean().sort_values(ascending=False) * 100).head(15)\n",
    "print('\\nTop 15 columns with missing values:')\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e34bd77",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "A common choice for this dataset is **RMSLE** (root mean squared log error).\n",
    "\n",
    "In practice, you can optimize RMSE on the transformed target: $\\log(1 + `SalePrice`)$.\n",
    "That is:\n",
    "- Train the model on `y_log = log1p(SalePrice)`\n",
    "- Evaluate RMSE on `y_log` using cross-validation\n",
    "- For final predictions, do `expm1(pred_log)` to get back to dollars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa822a6",
   "metadata": {},
   "source": [
    "## Data Cleaning Tips (Optional)\n",
    "\n",
    "Before moving to feature engineering, consider:\n",
    "\n",
    "1. **Drop high-missing columns** (e.g., >70% missing): `PoolQC`, `MiscFeature`, `Alley`, `Fence`, `FireplaceQu`\n",
    "   - Or understand what 'NA' means (often = 'No pool', 'No fence', etc.) and create binary indicators\n",
    "\n",
    "2. **Handle outliers**: Some very large/expensive houses can skew models\n",
    "   - Consider removing extreme outliers (e.g., GrLivArea > 4000 with low price)\n",
    "\n",
    "3. **Skewness correction**: Many numeric features are right-skewed\n",
    "   - Apply `log1p` or Box-Cox transformation to highly skewed features (skew > 0.75)\n",
    "\n",
    "4. **Feature types**: Check if some numeric columns should be categorical\n",
    "   - `MSSubClass`, `OverallQual`, `OverallCond` might work better as categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features/target\n",
    "X = train_df.drop(columns=['SalePrice'])\n",
    "y = train_df['SalePrice']\n",
    "\n",
    "# Optional (recommended): train on log1p target to align with RMSLE\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "X_train, X_valid, y_train_log, y_valid_log = train_test_split(\n",
    "    X, y_log, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print('X_train:', X_train.shape, 'X_valid:', X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f806d96b",
   "metadata": {},
   "source": [
    "## TODO 1: Build a leakage-safe preprocessing pipeline\n",
    "Use `ColumnTransformer` to preprocess numeric and categorical features.\n",
    "\n",
    "Suggested starting point:\n",
    "- Numeric: `SimpleImputer(strategy='median')`\n",
    "- Categorical: `SimpleImputer(strategy='most_frequent')` + `OneHotEncoder(handle_unknown='ignore')`\n",
    "\n",
    "Tip: Let pandas dtypes select columns automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Build a leakage-safe preprocessing pipeline\n",
    "\n",
    "# Goal: create a ColumnTransformer named `preprocess` that:\n",
    "# - imputes numeric features (median) and scales them with RobustScaler (robust to outliers)\n",
    "# - imputes categorical features (most_frequent or constant='Missing') then one-hot encodes\n",
    "# - uses `handle_unknown='ignore'` in OneHotEncoder\n",
    "\n",
    "# Advanced tips:\n",
    "# - RobustScaler is better than StandardScaler for data with outliers\n",
    "# - For high-missing columns, consider dropping them first or using constant imputation\n",
    "# - Apply skewness correction (log1p) to highly skewed numeric features before scaling\n",
    "\n",
    "# Hints:\n",
    "# - Start by identifying columns via pandas dtypes\n",
    "#   numeric_features = X_train.select_dtypes(exclude=['object']).columns\n",
    "#   categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "# - Use Pipeline + ColumnTransformer\n",
    "\n",
    "numeric_features = None  # TODO\n",
    "categorical_features = None  # TODO\n",
    "\n",
    "# Example structure (fill in the details):\n",
    "numeric_transformer = None  # TODO: Pipeline([('imputer', SimpleImputer(strategy='median')),\n",
    "#                                           ('scaler', RobustScaler())])\n",
    "\n",
    "categorical_transformer = None  # TODO: Pipeline([('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#                                                 ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocess = None  # TODO: ColumnTransformer([('num', numeric_transformer, numeric_features),\n",
    "#                                                ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "print(\"TODO: implement `preprocess` (ColumnTransformer) before moving on.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b9179b",
   "metadata": {},
   "source": [
    "## TODO 2: Baseline model (Linear Regression or Decision Tree)\n",
    "Build a baseline to establish a CV score before doing anything fancy.\n",
    "\n",
    "- Use a `Pipeline(preprocess -> LinearRegression)` or `Pipeline(preprocess -> DecisionTreeRegressor)`\n",
    "- Evaluate with 5-fold CV using RMSE on `y_log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: Baseline model (Linear Regression or Decision Tree)\n",
    "\n",
    "# Goal: build a Pipeline(preprocess -> LinearRegression or DecisionTreeRegressor) and evaluate CV RMSE on y_log.\n",
    "\n",
    "# Hints:\n",
    "# Option 1 - Linear Regression:\n",
    "# - model = Pipeline(steps=[('preprocess', preprocess), ('model', LinearRegression())])\n",
    "#\n",
    "# Option 2 - Decision Tree:\n",
    "# - model = Pipeline(steps=[('preprocess', preprocess), ('model', DecisionTreeRegressor(random_state=RANDOM_STATE))])\n",
    "#\n",
    "# Then use: cross_val_score(model, X, y_log, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# Convert to positive RMSE via the leading minus sign: rmse = -scores.mean()\n",
    "\n",
    "baseline_model = None  # TODO\n",
    "\n",
    "if preprocess is None:\n",
    "    print(\"TODO: implement `preprocess` first.\")\n",
    "else:\n",
    "    print(\"TODO: implement `baseline_model` and cross-validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dd50bb",
   "metadata": {},
   "source": [
    "## TODO 3: Creative Feature Engineering (your turn)\n",
    "Create **at least 3** new features that you believe improve predictive power.\n",
    "\n",
    "Ideas (pick a few):\n",
    "- Total square footage (basement + 1st + 2nd floor)\n",
    "- House age: `YrSold - YearBuilt`\n",
    "- Years since remodel: `YrSold - YearRemodAdd`\n",
    "- Total bathrooms: `FullBath + 0.5*HalfBath + BsmtFullBath + 0.5*BsmtHalfBath`\n",
    "- Indicator features: `HasGarage`, `HasBasement`, `HasFireplace`, `HasPool`\n",
    "\n",
    "**Important**: Feature engineering should not use `SalePrice` (avoid leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80b41df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton: write a function that adds engineered features\n",
    "def add_engineered_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add domain-specific engineered features.\n",
    "    \n",
    "    Best practices for housing price prediction:\n",
    "    1. Total area features (combine related areas)\n",
    "    2. Age/time features (house age, remodel age)\n",
    "    3. Quality aggregates (combine quality ratings)\n",
    "    4. Interaction features (area × quality)\n",
    "    5. Binary indicators (has feature or not)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # TODO: implement at least 5 engineered features below.\n",
    "    #\n",
    "    # Suggested features (high-impact):\n",
    "    #\n",
    "    # 1. TOTAL AREAS:\n",
    "    #    - TotalSF = TotalBsmtSF + 1stFlrSF + 2ndFlrSF\n",
    "    #    - TotalPorchSF = OpenPorchSF + EnclosedPorch + 3SsnPorch + ScreenPorch\n",
    "    #\n",
    "    # 2. AGE FEATURES:\n",
    "    #    - HouseAge = YrSold - YearBuilt\n",
    "    #    - YearsSinceRemodel = YrSold - YearRemodAdd\n",
    "    #    - GarageAge = YrSold - GarageYrBlt (watch for NaN)\n",
    "    #\n",
    "    # 3. QUALITY AGGREGATES:\n",
    "    #    - TotalBath = FullBath + 0.5*HalfBath + BsmtFullBath + 0.5*BsmtHalfBath\n",
    "    #    - TotalQual = OverallQual + OverallCond\n",
    "    #\n",
    "    # 4. BINARY INDICATORS:\n",
    "    #    - HasGarage = (GarageArea > 0).astype(int)\n",
    "    #    - HasBasement = (TotalBsmtSF > 0).astype(int)\n",
    "    #    - HasFireplace = (Fireplaces > 0).astype(int)\n",
    "    #    - Has2ndFloor = (2ndFlrSF > 0).astype(int)\n",
    "    #    - HasPool = (PoolArea > 0).astype(int)\n",
    "    #\n",
    "    # 5. INTERACTION FEATURES (area × quality):\n",
    "    #    - QualityTimesArea = OverallQual × GrLivArea\n",
    "    #\n",
    "    # 6. POLYNOMIAL FEATURES (for key predictors):\n",
    "    #    - GrLivArea_Squared = GrLivArea²\n",
    "    \n",
    "    return df\n",
    "\n",
    "# NOTE: Once implemented, sanity-check shape changes:\n",
    "# X_fe = add_engineered_features(X_train)\n",
    "# print('Before:', X_train.shape, 'After:', X_fe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2febf68",
   "metadata": {},
   "source": [
    "## TODO 4: Advanced regression model — Random Forest\n",
    "Train a Random Forest model (with preprocessing!) and evaluate with CV.\n",
    "\n",
    "Tips:\n",
    "- Random Forest doesn’t need feature scaling, but it benefits from good missing-value handling and encoded categoricals.\n",
    "- Start simple, then tune: `n_estimators`, `max_depth`, `min_samples_leaf`, `max_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f595b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4: Advanced regression model — Random Forest\n",
    "\n",
    "# Goal: build a Pipeline(preprocess -> RandomForestRegressor) and evaluate CV RMSE on y_log.\n",
    "\n",
    "# Hints:\n",
    "# - rf = RandomForestRegressor(random_state=RANDOM_STATE, n_estimators=..., max_depth=..., n_jobs=-1)\n",
    "# - rf_model = Pipeline(steps=[('preprocess', preprocess), ('model', rf)])\n",
    "# - Use: cross_val_score(rf_model, X, y_log, cv=5, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "rf_model = None  # TODO\n",
    "\n",
    "if preprocess is None:\n",
    "    print(\"TODO: implement `preprocess` first.\")\n",
    "else:\n",
    "    print(\"TODO: implement `rf_model` and cross-validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663692a",
   "metadata": {},
   "source": [
    "## TODO 5: Hyperparameter tuning (RandomizedSearchCV)\n",
    "Use `RandomizedSearchCV` to tune your Random Forest (or Decision Tree).\n",
    "\n",
    "Deliverable:\n",
    "- Print best CV score\n",
    "- Print best params\n",
    "- Keep a reference to the best estimator as `best_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6602427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# TODO 5: Hyperparameter tuning (RandomizedSearchCV)\n",
    "\n",
    "# Goal: run RandomizedSearchCV on your model to find better hyperparameters.\n",
    "\n",
    "# Hints:\n",
    "# - Use param names like 'model__n_estimators' because the regressor is inside Pipeline step 'model'\n",
    "# - Keep n_iter modest to save time (e.g., 10–30)\n",
    "# - Can tune DecisionTree (model__max_depth, model__min_samples_split) or \n",
    "#   RandomForest (model__n_estimators, model__max_depth, etc.)\n",
    "\n",
    "# Example parameter grid for Random Forest:\n",
    "param_distributions = {\n",
    "    # TODO: fill in appropriate values\n",
    "    # 'model__n_estimators': [100, 200, 500],\n",
    "    # 'model__max_depth': [None, 10, 20, 30],\n",
    "    # 'model__min_samples_leaf': [1, 2, 4],\n",
    "    # 'model__max_features': ['sqrt', 'log2', None],\n",
    "}\n",
    "\n",
    "search = None  # TODO: RandomizedSearchCV(estimator=..., param_distributions=..., n_iter=20, cv=5, ...)\n",
    "\n",
    "# TODO: run the search when ready\n",
    "# search.fit(X, y_log)\n",
    "# print('Best CV RMSE (log target):', -search.best_score_)\n",
    "# print('Best params:', search.best_params_)\n",
    "# best_model = search.best_estimator_\n",
    "\n",
    "best_model = None\n",
    "\n",
    "print(\"TODO: complete RandomizedSearchCV and set `best_model`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197b350",
   "metadata": {},
   "source": [
    "## TODO 6: Final Model Evaluation\n",
    "Once you have a `best_model`, evaluate it on the validation set to get final performance metrics.\n",
    "\n",
    "Optional: You can also predict on the test set to see how your model generalizes to completely unseen data (though you won't have true labels to compare against)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5adc12",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 6: Final model evaluation\n",
    "\n",
    "# Once you have a `best_model`, evaluate it properly\n",
    "\n",
    "if best_model is None:\n",
    "    print('TODO: set best_model (e.g., from RandomizedSearchCV) first.')\n",
    "else:\n",
    "    # Option 1: Evaluate on validation set\n",
    "    best_model.fit(X_train, y_train_log)\n",
    "    y_pred_log = best_model.predict(X_valid)\n",
    "    \n",
    "    # Calculate RMSE on log scale\n",
    "    rmse_log = np.sqrt(mean_squared_error(y_valid_log, y_pred_log))\n",
    "    print(f'Validation RMSE (log scale): {rmse_log:.4f}')\n",
    "    \n",
    "    # Convert back to original scale for interpretation\n",
    "    y_valid_original = np.expm1(y_valid_log)\n",
    "    y_pred_original = np.expm1(y_pred_log)\n",
    "    rmse_original = np.sqrt(mean_squared_error(y_valid_original, y_pred_original))\n",
    "    print(f'Validation RMSE (original scale): ${rmse_original:,.2f}')\n",
    "    \n",
    "    # Option 2: Train on full data and evaluate with CV\n",
    "    # final_cv_score = cross_val_score(best_model, X, y_log, cv=5, \n",
    "    #                                   scoring='neg_root_mean_squared_error').mean()\n",
    "    # print(f'Final CV RMSE: {-final_cv_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ce96de",
   "metadata": {},
   "source": [
    "## Optional extensions (extra challenge)\n",
    "\n",
    "If you finish early and want to push further (still using only the techniques from this workshop), try:\n",
    "\n",
    "### 1. Skewness Correction\n",
    "- Identify highly skewed numeric features (skew > 0.75)\n",
    "- Apply `log1p` transformation before preprocessing\n",
    "\n",
    "### 2. Feature Selection (simple + practical)\n",
    "- Drop high-missing columns (>70% missing)\n",
    "- Use Random Forest feature importances to decide what to keep/drop\n",
    "\n",
    "### 3. Ensembling (simple averaging)\n",
    "- Train multiple models (Linear Regression, Decision Tree, RandomForest)\n",
    "- Average predictions from multiple models\n",
    "\n",
    "### 4. Outlier Handling\n",
    "- Remove extreme outliers (e.g., very large living area with unusually low price)\n",
    "- Compare CV score before/after removing outliers\n",
    "\n",
    "### 5. Cross-Validation Strategy\n",
    "- Use KFold with more folds (e.g., 10-fold) for more stable estimates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
